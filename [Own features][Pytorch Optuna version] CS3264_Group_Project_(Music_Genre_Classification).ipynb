{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_simple.py\n",
    "\n",
    "\"\"\"\n",
    "Optuna example that optimizes multi-layer perceptrons using PyTorch.\n",
    "\n",
    "In this example, we optimize the validation accuracy of fashion product recognition using\n",
    "PyTorch and FashionMNIST. We optimize the neural network architecture as well as the optimizer\n",
    "configuration. As it is too time consuming to use the whole FashionMNIST dataset,\n",
    "we here use a small subset of it.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and checking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chroma_stft_mean</th>\n",
       "      <th>chroma_stft_var</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>spectral_centroid_mean</th>\n",
       "      <th>spectral_centroid_var</th>\n",
       "      <th>spectral_bandwidth_mean</th>\n",
       "      <th>spectral_bandwidth_var</th>\n",
       "      <th>rolloff_mean</th>\n",
       "      <th>rolloff_var</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc16_var</th>\n",
       "      <th>mfcc17_mean</th>\n",
       "      <th>mfcc17_var</th>\n",
       "      <th>mfcc18_mean</th>\n",
       "      <th>mfcc18_var</th>\n",
       "      <th>mfcc19_mean</th>\n",
       "      <th>mfcc19_var</th>\n",
       "      <th>mfcc20_mean</th>\n",
       "      <th>mfcc20_var</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.350129</td>\n",
       "      <td>0.088772</td>\n",
       "      <td>0.130184</td>\n",
       "      <td>0.002828</td>\n",
       "      <td>1784.122641</td>\n",
       "      <td>129745.484419</td>\n",
       "      <td>2002.412407</td>\n",
       "      <td>85834.410406</td>\n",
       "      <td>3805.723030</td>\n",
       "      <td>9.012529e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>52.424534</td>\n",
       "      <td>-1.687854</td>\n",
       "      <td>36.535866</td>\n",
       "      <td>-0.408730</td>\n",
       "      <td>41.603172</td>\n",
       "      <td>-2.302677</td>\n",
       "      <td>55.053654</td>\n",
       "      <td>1.222467</td>\n",
       "      <td>46.941349</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.340849</td>\n",
       "      <td>0.094976</td>\n",
       "      <td>0.095908</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>1530.261767</td>\n",
       "      <td>375915.508778</td>\n",
       "      <td>2038.987608</td>\n",
       "      <td>213905.103191</td>\n",
       "      <td>3550.713616</td>\n",
       "      <td>2.978311e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>55.337963</td>\n",
       "      <td>-0.728403</td>\n",
       "      <td>60.231407</td>\n",
       "      <td>0.296872</td>\n",
       "      <td>48.133213</td>\n",
       "      <td>-0.282430</td>\n",
       "      <td>51.106014</td>\n",
       "      <td>0.530644</td>\n",
       "      <td>45.788700</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.363538</td>\n",
       "      <td>0.085257</td>\n",
       "      <td>0.175473</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>1552.832481</td>\n",
       "      <td>156471.010904</td>\n",
       "      <td>1747.754087</td>\n",
       "      <td>76295.413398</td>\n",
       "      <td>3042.410115</td>\n",
       "      <td>7.841309e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>40.641678</td>\n",
       "      <td>-7.724839</td>\n",
       "      <td>47.629646</td>\n",
       "      <td>-1.819024</td>\n",
       "      <td>52.393604</td>\n",
       "      <td>-3.440457</td>\n",
       "      <td>46.643394</td>\n",
       "      <td>-2.238127</td>\n",
       "      <td>30.653151</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.404854</td>\n",
       "      <td>0.093999</td>\n",
       "      <td>0.141040</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>1070.153418</td>\n",
       "      <td>184366.009438</td>\n",
       "      <td>1596.422564</td>\n",
       "      <td>166551.844243</td>\n",
       "      <td>2184.879029</td>\n",
       "      <td>1.493078e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>44.432903</td>\n",
       "      <td>-3.324069</td>\n",
       "      <td>50.218452</td>\n",
       "      <td>0.636311</td>\n",
       "      <td>37.325726</td>\n",
       "      <td>-0.615968</td>\n",
       "      <td>37.257774</td>\n",
       "      <td>-3.405046</td>\n",
       "      <td>31.965258</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.308526</td>\n",
       "      <td>0.087843</td>\n",
       "      <td>0.091501</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>1835.128513</td>\n",
       "      <td>343249.495746</td>\n",
       "      <td>1748.410758</td>\n",
       "      <td>88378.704478</td>\n",
       "      <td>3579.957471</td>\n",
       "      <td>1.572336e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>85.995201</td>\n",
       "      <td>-5.451786</td>\n",
       "      <td>75.276741</td>\n",
       "      <td>-0.915952</td>\n",
       "      <td>53.633236</td>\n",
       "      <td>-4.408018</td>\n",
       "      <td>62.882484</td>\n",
       "      <td>-11.704385</td>\n",
       "      <td>55.190254</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   chroma_stft_mean  chroma_stft_var  rms_mean   rms_var  \\\n",
       "0          0.350129         0.088772  0.130184  0.002828   \n",
       "1          0.340849         0.094976  0.095908  0.002373   \n",
       "2          0.363538         0.085257  0.175473  0.002751   \n",
       "3          0.404854         0.093999  0.141040  0.006348   \n",
       "4          0.308526         0.087843  0.091501  0.002305   \n",
       "\n",
       "   spectral_centroid_mean  spectral_centroid_var  spectral_bandwidth_mean  \\\n",
       "0             1784.122641          129745.484419              2002.412407   \n",
       "1             1530.261767          375915.508778              2038.987608   \n",
       "2             1552.832481          156471.010904              1747.754087   \n",
       "3             1070.153418          184366.009438              1596.422564   \n",
       "4             1835.128513          343249.495746              1748.410758   \n",
       "\n",
       "   spectral_bandwidth_var  rolloff_mean   rolloff_var  ...  mfcc16_var  \\\n",
       "0            85834.410406   3805.723030  9.012529e+05  ...   52.424534   \n",
       "1           213905.103191   3550.713616  2.978311e+06  ...   55.337963   \n",
       "2            76295.413398   3042.410115  7.841309e+05  ...   40.641678   \n",
       "3           166551.844243   2184.879029  1.493078e+06  ...   44.432903   \n",
       "4            88378.704478   3579.957471  1.572336e+06  ...   85.995201   \n",
       "\n",
       "   mfcc17_mean  mfcc17_var  mfcc18_mean  mfcc18_var  mfcc19_mean  mfcc19_var  \\\n",
       "0    -1.687854   36.535866    -0.408730   41.603172    -2.302677   55.053654   \n",
       "1    -0.728403   60.231407     0.296872   48.133213    -0.282430   51.106014   \n",
       "2    -7.724839   47.629646    -1.819024   52.393604    -3.440457   46.643394   \n",
       "3    -3.324069   50.218452     0.636311   37.325726    -0.615968   37.257774   \n",
       "4    -5.451786   75.276741    -0.915952   53.633236    -4.408018   62.882484   \n",
       "\n",
       "   mfcc20_mean  mfcc20_var  label  \n",
       "0     1.222467   46.941349  blues  \n",
       "1     0.530644   45.788700  blues  \n",
       "2    -2.238127   30.653151  blues  \n",
       "3    -3.405046   31.965258  blues  \n",
       "4   -11.704385   55.190254  blues  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\User\\Downloads\\CS3264\\music genre\\feature.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting y and X from the df, where y is the label column, and X is all other columns\n",
    "\n",
    "y = df[\"label\"]\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X = df.iloc[:, :-1]\n",
    "\n",
    "# X.head()\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X to numpy array, y is already numpy array\n",
    "\n",
    "X = X.values\n",
    "# y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=34)\n",
    "# print(X_train)\n",
    "# print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising values of features so that every value is between 0 and 1\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(X_train)\n",
    "X_test = min_max_scaler.transform(X_test)\n",
    "\n",
    "# print(X_test_scaled_arr)\n",
    "# print(type(X_train))\n",
    "# print(type(y_train))\n",
    "# print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X features to float tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "# print(X_train)\n",
    "# Convert y labels to tensors long\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to(DEVICE)\n",
    "X_test = X_test.to(DEVICE)\n",
    "y_train = y_train.to(DEVICE)\n",
    "y_test = y_test.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([899, 57])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 57])\n",
      "torch.Size([3])\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# https://discuss.pytorch.org/t/how-to-split-your-training-data-into-indexable-batches/141705\n",
    "# Splitting training data into batches for backwards propagation, because seems to be able to prevent overfitting and better generalisation:\n",
    "    # https://datascience.stackexchange.com/questions/16807/why-mini-batch-size-is-better-than-one-single-batch-with-all-training-data\n",
    "\n",
    "batch_size = 32\n",
    "num_batches = math.ceil(X_train.size()[0]/batch_size)\n",
    "X_train_subsets = [X_train[batch_size*i:batch_size*(i+1),:] for i in range(num_batches)]\n",
    "y_train_subsets = [y_train[batch_size*i:batch_size*(i+1)] for i in range(num_batches)]\n",
    "print(X_train_subsets[28].size())\n",
    "print(y_train_subsets[28].size())\n",
    "print(len(X_train_subsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCHSIZE = 128\n",
    "CLASSES = 10\n",
    "# DIR = os.getcwd()\n",
    "# EPOCHS = 1500\n",
    "# N_TRAIN_EXAMPLES = BATCHSIZE * 30\n",
    "# N_VALID_EXAMPLES = BATCHSIZE * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating model using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 57\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 100, 500)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.1, 0.7)\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, CLASSES))\n",
    "    # layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Generate the model.\n",
    "    model = define_model(trial).to(DEVICE)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    EPOCHS = trial.suggest_int(\"EPOCHS\", 100, 2000)\n",
    "\n",
    "    # Train our model!\n",
    "    # Epochs? (one run thru all the training data in our network)\n",
    "    for i in range(EPOCHS):\n",
    "        model.train()\n",
    "        for j in range(len(X_train_subsets)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_train_subsets[j])\n",
    "            # loss = F.nll_loss(y_pred, y_train)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(y_pred, y_train_subsets[j])\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "\n",
    "        # print every 10 epoch\n",
    "        # if i % 10 == 0:\n",
    "        #     print(f'Epoch: {i} and training loss: {loss}')\n",
    "\n",
    "\n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            output = model(X_test)\n",
    "            # Get the index of the max log-probability.\n",
    "            y_test_pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += y_test_pred.eq(y_test.view_as(y_test_pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / len(y_test)\n",
    "\n",
    "        trial.report(accuracy, i)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-19 14:56:05,363] A new study created in memory with name: no-name-58a02c3a-692f-4440-ac5a-ae87b4151218\n",
      "[I 2024-04-19 14:57:52,365] Trial 0 finished with value: 0.1 and parameters: {'n_layers': 4, 'n_units_l0': 331, 'dropout_l0': 0.17995698377260583, 'n_units_l1': 459, 'dropout_l1': 0.38271999862853934, 'n_units_l2': 286, 'dropout_l2': 0.6203134683942118, 'n_units_l3': 416, 'dropout_l3': 0.41715411493516297, 'optimizer': 'SGD', 'lr': 0.00017694457559244822, 'EPOCHS': 1632}. Best is trial 0 with value: 0.1.\n",
      "[I 2024-04-19 14:58:39,950] Trial 1 finished with value: 0.7 and parameters: {'n_layers': 1, 'n_units_l0': 202, 'dropout_l0': 0.3058832376397115, 'optimizer': 'SGD', 'lr': 0.002599398166336402, 'EPOCHS': 1525}. Best is trial 1 with value: 0.7.\n",
      "[I 2024-04-19 14:58:45,250] Trial 2 finished with value: 0.4 and parameters: {'n_layers': 2, 'n_units_l0': 351, 'dropout_l0': 0.6443030712399052, 'n_units_l1': 114, 'dropout_l1': 0.6338049902423619, 'optimizer': 'Adam', 'lr': 4.523791007703372e-05, 'EPOCHS': 103}. Best is trial 1 with value: 0.7.\n",
      "[I 2024-04-19 14:58:52,148] Trial 3 finished with value: 0.1 and parameters: {'n_layers': 3, 'n_units_l0': 188, 'dropout_l0': 0.5093012000265316, 'n_units_l1': 197, 'dropout_l1': 0.3384295733649184, 'n_units_l2': 181, 'dropout_l2': 0.2345650453862736, 'optimizer': 'SGD', 'lr': 0.00044046305552270036, 'EPOCHS': 125}. Best is trial 1 with value: 0.7.\n",
      "[I 2024-04-19 14:59:20,578] Trial 4 finished with value: 0.1 and parameters: {'n_layers': 4, 'n_units_l0': 139, 'dropout_l0': 0.5670185433068284, 'n_units_l1': 246, 'dropout_l1': 0.5188840255332688, 'n_units_l2': 256, 'dropout_l2': 0.1196611470622106, 'n_units_l3': 462, 'dropout_l3': 0.24384546526260703, 'optimizer': 'Adam', 'lr': 0.031060967486837458, 'EPOCHS': 384}. Best is trial 1 with value: 0.7.\n",
      "[I 2024-04-19 14:59:27,901] Trial 5 finished with value: 0.71 and parameters: {'n_layers': 1, 'n_units_l0': 428, 'dropout_l0': 0.10289941846039588, 'optimizer': 'RMSprop', 'lr': 0.00026350015948612353, 'EPOCHS': 211}. Best is trial 5 with value: 0.71.\n",
      "[I 2024-04-19 14:59:27,943] Trial 6 pruned. \n",
      "[I 2024-04-19 14:59:28,002] Trial 7 pruned. \n",
      "[I 2024-04-19 14:59:37,629] Trial 8 finished with value: 0.73 and parameters: {'n_layers': 2, 'n_units_l0': 101, 'dropout_l0': 0.23262990827818664, 'n_units_l1': 234, 'dropout_l1': 0.4012958585254881, 'optimizer': 'RMSprop', 'lr': 0.000524426314659592, 'EPOCHS': 211}. Best is trial 8 with value: 0.73.\n",
      "[I 2024-04-19 14:59:37,686] Trial 9 pruned. \n",
      "[I 2024-04-19 15:00:20,568] Trial 10 finished with value: 0.49 and parameters: {'n_layers': 2, 'n_units_l0': 495, 'dropout_l0': 0.3776728217177943, 'n_units_l1': 352, 'dropout_l1': 0.5241725172109138, 'optimizer': 'RMSprop', 'lr': 0.014049975596116704, 'EPOCHS': 925}. Best is trial 8 with value: 0.73.\n",
      "[I 2024-04-19 15:00:20,625] Trial 11 pruned. \n",
      "[I 2024-04-19 15:01:08,165] Trial 12 finished with value: 0.78 and parameters: {'n_layers': 1, 'n_units_l0': 406, 'dropout_l0': 0.20159019907105122, 'optimizer': 'RMSprop', 'lr': 0.00015117319998869452, 'EPOCHS': 1239}. Best is trial 12 with value: 0.78.\n",
      "[I 2024-04-19 15:01:08,237] Trial 13 pruned. \n",
      "[I 2024-04-19 15:01:08,361] Trial 14 pruned. \n",
      "[I 2024-04-19 15:01:31,979] Trial 15 finished with value: 0.8 and parameters: {'n_layers': 1, 'n_units_l0': 285, 'dropout_l0': 0.4116151961776455, 'optimizer': 'RMSprop', 'lr': 0.0008618087234375442, 'EPOCHS': 699}. Best is trial 15 with value: 0.8.\n",
      "[I 2024-04-19 15:01:32,905] Trial 16 pruned. \n",
      "[I 2024-04-19 15:02:09,595] Trial 17 finished with value: 0.76 and parameters: {'n_layers': 1, 'n_units_l0': 279, 'dropout_l0': 0.4034081156567847, 'optimizer': 'RMSprop', 'lr': 0.0015881730246845532, 'EPOCHS': 1038}. Best is trial 15 with value: 0.8.\n",
      "[I 2024-04-19 15:02:09,653] Trial 18 pruned. \n",
      "[I 2024-04-19 15:02:09,737] Trial 19 pruned. \n",
      "[I 2024-04-19 15:02:54,182] Trial 20 finished with value: 0.73 and parameters: {'n_layers': 1, 'n_units_l0': 495, 'dropout_l0': 0.3834201623082341, 'optimizer': 'RMSprop', 'lr': 0.006453385287859039, 'EPOCHS': 1278}. Best is trial 15 with value: 0.8.\n",
      "[I 2024-04-19 15:03:32,020] Trial 21 finished with value: 0.74 and parameters: {'n_layers': 1, 'n_units_l0': 283, 'dropout_l0': 0.4420144269943468, 'optimizer': 'RMSprop', 'lr': 0.0014118826261174328, 'EPOCHS': 1065}. Best is trial 15 with value: 0.8.\n",
      "[I 2024-04-19 15:04:08,830] Trial 22 finished with value: 0.78 and parameters: {'n_layers': 1, 'n_units_l0': 303, 'dropout_l0': 0.3663577260011456, 'optimizer': 'RMSprop', 'lr': 0.0009394294193398696, 'EPOCHS': 1029}. Best is trial 15 with value: 0.8.\n",
      "[I 2024-04-19 15:04:08,893] Trial 23 pruned. \n",
      "[I 2024-04-19 15:04:51,226] Trial 24 finished with value: 0.78 and parameters: {'n_layers': 2, 'n_units_l0': 302, 'dropout_l0': 0.2902655955524738, 'n_units_l1': 350, 'dropout_l1': 0.5047855022584098, 'optimizer': 'RMSprop', 'lr': 0.000659941311175139, 'EPOCHS': 896}. Best is trial 15 with value: 0.8.\n",
      "[I 2024-04-19 15:04:51,317] Trial 25 pruned. \n",
      "[I 2024-04-19 15:04:51,397] Trial 26 pruned. \n",
      "[I 2024-04-19 15:05:41,335] Trial 27 finished with value: 0.76 and parameters: {'n_layers': 1, 'n_units_l0': 376, 'dropout_l0': 0.2745275260118475, 'optimizer': 'RMSprop', 'lr': 0.0007011724645560062, 'EPOCHS': 1405}. Best is trial 15 with value: 0.8.\n",
      "[I 2024-04-19 15:05:41,434] Trial 28 pruned. \n",
      "[I 2024-04-19 15:05:41,546] Trial 29 pruned. \n",
      "[I 2024-04-19 15:05:41,602] Trial 30 pruned. \n",
      "[I 2024-04-19 15:06:25,844] Trial 31 finished with value: 0.78 and parameters: {'n_layers': 2, 'n_units_l0': 303, 'dropout_l0': 0.2636893254396957, 'n_units_l1': 371, 'dropout_l1': 0.5179883735523432, 'optimizer': 'RMSprop', 'lr': 0.000814192856198385, 'EPOCHS': 929}. Best is trial 15 with value: 0.8.\n",
      "[I 2024-04-19 15:06:25,976] Trial 32 pruned. \n",
      "[I 2024-04-19 15:06:26,283] Trial 33 pruned. \n",
      "[I 2024-04-19 15:07:41,092] Trial 34 finished with value: 0.76 and parameters: {'n_layers': 2, 'n_units_l0': 396, 'dropout_l0': 0.3378916944251763, 'n_units_l1': 409, 'dropout_l1': 0.58443578733656, 'optimizer': 'RMSprop', 'lr': 0.00038002214697242803, 'EPOCHS': 1554}. Best is trial 15 with value: 0.8.\n",
      "[I 2024-04-19 15:07:41,148] Trial 35 pruned. \n",
      "[I 2024-04-19 15:08:12,108] Trial 36 finished with value: 0.8 and parameters: {'n_layers': 2, 'n_units_l0': 324, 'dropout_l0': 0.2914600632872549, 'n_units_l1': 327, 'dropout_l1': 0.3001118346271404, 'optimizer': 'Adam', 'lr': 0.00119900823982225, 'EPOCHS': 602}. Best is trial 15 with value: 0.8.\n",
      "[I 2024-04-19 15:08:33,540] Trial 37 finished with value: 0.78 and parameters: {'n_layers': 1, 'n_units_l0': 331, 'dropout_l0': 0.1500845058513171, 'optimizer': 'Adam', 'lr': 0.0025805746021216997, 'EPOCHS': 552}. Best is trial 15 with value: 0.8.\n",
      "[I 2024-04-19 15:08:33,711] Trial 38 pruned. \n",
      "[I 2024-04-19 15:08:33,836] Trial 39 pruned. \n",
      "[I 2024-04-19 15:08:33,902] Trial 40 pruned. \n",
      "[I 2024-04-19 15:08:34,092] Trial 41 pruned. \n",
      "[I 2024-04-19 15:08:34,221] Trial 42 pruned. \n",
      "[I 2024-04-19 15:08:34,623] Trial 43 pruned. \n",
      "[I 2024-04-19 15:08:34,704] Trial 44 pruned. \n",
      "[I 2024-04-19 15:08:34,860] Trial 45 pruned. \n",
      "[I 2024-04-19 15:08:34,921] Trial 46 pruned. \n",
      "[I 2024-04-19 15:08:35,185] Trial 47 pruned. \n",
      "[I 2024-04-19 15:08:35,247] Trial 48 pruned. \n",
      "[I 2024-04-19 15:08:35,377] Trial 49 pruned. \n",
      "[I 2024-04-19 15:08:35,436] Trial 50 pruned. \n",
      "[I 2024-04-19 15:08:35,566] Trial 51 pruned. \n",
      "[I 2024-04-19 15:08:35,689] Trial 52 pruned. \n",
      "[I 2024-04-19 15:08:35,820] Trial 53 pruned. \n",
      "[I 2024-04-19 15:08:35,929] Trial 54 pruned. \n",
      "[I 2024-04-19 15:08:36,013] Trial 55 pruned. \n",
      "[I 2024-04-19 15:08:36,075] Trial 56 pruned. \n",
      "[I 2024-04-19 15:08:36,369] Trial 57 pruned. \n",
      "[I 2024-04-19 15:08:36,431] Trial 58 pruned. \n",
      "[I 2024-04-19 15:08:36,515] Trial 59 pruned. \n",
      "[I 2024-04-19 15:08:59,251] Trial 60 finished with value: 0.76 and parameters: {'n_layers': 1, 'n_units_l0': 176, 'dropout_l0': 0.37764261697801804, 'optimizer': 'Adam', 'lr': 0.008989278768498503, 'EPOCHS': 590}. Best is trial 15 with value: 0.8.\n",
      "[I 2024-04-19 15:09:19,964] Trial 61 finished with value: 0.82 and parameters: {'n_layers': 1, 'n_units_l0': 333, 'dropout_l0': 0.17625976990536513, 'optimizer': 'Adam', 'lr': 0.0024732112937603562, 'EPOCHS': 526}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:09:37,903] Trial 62 finished with value: 0.78 and parameters: {'n_layers': 1, 'n_units_l0': 359, 'dropout_l0': 0.16227379149896284, 'optimizer': 'Adam', 'lr': 0.0011236960479514022, 'EPOCHS': 462}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:10:06,123] Trial 63 finished with value: 0.77 and parameters: {'n_layers': 1, 'n_units_l0': 387, 'dropout_l0': 0.12479471796129277, 'optimizer': 'Adam', 'lr': 0.0018205557411711878, 'EPOCHS': 700}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:10:06,196] Trial 64 pruned. \n",
      "[I 2024-04-19 15:10:06,568] Trial 65 pruned. \n",
      "[I 2024-04-19 15:10:06,639] Trial 66 pruned. \n",
      "[I 2024-04-19 15:10:06,724] Trial 67 pruned. \n",
      "[I 2024-04-19 15:10:06,791] Trial 68 pruned. \n",
      "[I 2024-04-19 15:10:34,524] Trial 69 finished with value: 0.76 and parameters: {'n_layers': 1, 'n_units_l0': 233, 'dropout_l0': 0.1518170154592449, 'optimizer': 'Adam', 'lr': 0.005943339224170698, 'EPOCHS': 707}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:10:34,605] Trial 70 pruned. \n",
      "[I 2024-04-19 15:10:56,263] Trial 71 finished with value: 0.75 and parameters: {'n_layers': 1, 'n_units_l0': 334, 'dropout_l0': 0.10620460897613115, 'optimizer': 'Adam', 'lr': 0.002221671890921818, 'EPOCHS': 553}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:11:16,601] Trial 72 finished with value: 0.78 and parameters: {'n_layers': 1, 'n_units_l0': 367, 'dropout_l0': 0.1617167109578854, 'optimizer': 'Adam', 'lr': 0.0028138401686918567, 'EPOCHS': 516}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:11:30,043] Trial 73 finished with value: 0.7 and parameters: {'n_layers': 1, 'n_units_l0': 308, 'dropout_l0': 0.12602341992522678, 'optimizer': 'Adam', 'lr': 0.0101472078625231, 'EPOCHS': 339}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:12:03,091] Trial 74 finished with value: 0.79 and parameters: {'n_layers': 1, 'n_units_l0': 324, 'dropout_l0': 0.15477333875389154, 'optimizer': 'Adam', 'lr': 0.0008760271350848618, 'EPOCHS': 842}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:12:03,158] Trial 75 pruned. \n",
      "[I 2024-04-19 15:12:03,224] Trial 76 pruned. \n",
      "[I 2024-04-19 15:12:29,911] Trial 77 finished with value: 0.76 and parameters: {'n_layers': 1, 'n_units_l0': 349, 'dropout_l0': 0.23647474227314458, 'optimizer': 'Adam', 'lr': 0.0015183254237074333, 'EPOCHS': 683}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:12:29,995] Trial 78 pruned. \n",
      "[I 2024-04-19 15:12:30,058] Trial 79 pruned. \n",
      "[I 2024-04-19 15:12:30,168] Trial 80 pruned. \n",
      "[I 2024-04-19 15:12:52,987] Trial 81 finished with value: 0.8 and parameters: {'n_layers': 1, 'n_units_l0': 329, 'dropout_l0': 0.14434129032772153, 'optimizer': 'Adam', 'lr': 0.002730545581593012, 'EPOCHS': 573}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:13:17,916] Trial 82 finished with value: 0.76 and parameters: {'n_layers': 1, 'n_units_l0': 308, 'dropout_l0': 0.14173986740492883, 'optimizer': 'Adam', 'lr': 0.002115403534958108, 'EPOCHS': 633}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:13:46,633] Trial 83 finished with value: 0.74 and parameters: {'n_layers': 1, 'n_units_l0': 279, 'dropout_l0': 0.6272895395719214, 'optimizer': 'Adam', 'lr': 0.005105071338398996, 'EPOCHS': 738}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:14:21,034] Trial 84 finished with value: 0.78 and parameters: {'n_layers': 1, 'n_units_l0': 326, 'dropout_l0': 0.17748274286048052, 'optimizer': 'Adam', 'lr': 0.0014318263284120723, 'EPOCHS': 862}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:14:43,755] Trial 85 finished with value: 0.78 and parameters: {'n_layers': 1, 'n_units_l0': 336, 'dropout_l0': 0.440094007546067, 'optimizer': 'Adam', 'lr': 0.003099371687407286, 'EPOCHS': 580}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:14:43,845] Trial 86 pruned. \n",
      "[I 2024-04-19 15:14:43,911] Trial 87 pruned. \n",
      "[I 2024-04-19 15:14:44,035] Trial 88 pruned. \n",
      "[I 2024-04-19 15:15:18,789] Trial 89 finished with value: 0.77 and parameters: {'n_layers': 2, 'n_units_l0': 390, 'dropout_l0': 0.35804839349887335, 'n_units_l1': 448, 'dropout_l1': 0.33185572917823536, 'optimizer': 'Adam', 'lr': 0.0010765280898443396, 'EPOCHS': 670}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:15:18,861] Trial 90 pruned. \n",
      "[I 2024-04-19 15:15:57,035] Trial 91 finished with value: 0.77 and parameters: {'n_layers': 1, 'n_units_l0': 342, 'dropout_l0': 0.14105205556811723, 'optimizer': 'Adam', 'lr': 0.0031547936804064538, 'EPOCHS': 992}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:16:19,889] Trial 92 finished with value: 0.73 and parameters: {'n_layers': 1, 'n_units_l0': 316, 'dropout_l0': 0.19147125431556533, 'optimizer': 'Adam', 'lr': 0.014887336662405617, 'EPOCHS': 576}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:16:44,887] Trial 93 finished with value: 0.75 and parameters: {'n_layers': 1, 'n_units_l0': 328, 'dropout_l0': 0.1014662836330196, 'optimizer': 'Adam', 'lr': 0.0023362566724382757, 'EPOCHS': 641}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:16:44,961] Trial 94 pruned. \n",
      "[I 2024-04-19 15:16:45,067] Trial 95 pruned. \n",
      "[I 2024-04-19 15:16:45,131] Trial 96 pruned. \n",
      "[I 2024-04-19 15:16:45,219] Trial 97 pruned. \n",
      "[I 2024-04-19 15:16:45,290] Trial 98 pruned. \n",
      "[I 2024-04-19 15:16:45,356] Trial 99 pruned. \n",
      "[I 2024-04-19 15:16:45,445] Trial 100 pruned. \n",
      "[I 2024-04-19 15:16:45,589] Trial 101 pruned. \n",
      "[I 2024-04-19 15:16:45,769] Trial 102 pruned. \n",
      "[I 2024-04-19 15:17:04,926] Trial 103 finished with value: 0.77 and parameters: {'n_layers': 1, 'n_units_l0': 401, 'dropout_l0': 0.34464510150052907, 'optimizer': 'Adam', 'lr': 0.0019265698573652191, 'EPOCHS': 504}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:17:26,557] Trial 104 finished with value: 0.79 and parameters: {'n_layers': 1, 'n_units_l0': 338, 'dropout_l0': 0.19855658635095697, 'optimizer': 'Adam', 'lr': 0.0024354291537620866, 'EPOCHS': 567}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:17:26,636] Trial 105 pruned. \n",
      "[I 2024-04-19 15:17:26,709] Trial 106 pruned. \n",
      "[I 2024-04-19 15:17:26,777] Trial 107 pruned. \n",
      "[I 2024-04-19 15:17:26,871] Trial 108 pruned. \n",
      "[I 2024-04-19 15:17:26,940] Trial 109 pruned. \n",
      "[I 2024-04-19 15:17:27,052] Trial 110 pruned. \n",
      "[I 2024-04-19 15:17:27,290] Trial 111 pruned. \n",
      "[I 2024-04-19 15:17:27,443] Trial 112 pruned. \n",
      "[I 2024-04-19 15:17:27,632] Trial 113 pruned. \n",
      "[I 2024-04-19 15:17:27,707] Trial 114 pruned. \n",
      "[I 2024-04-19 15:17:27,778] Trial 115 pruned. \n",
      "[I 2024-04-19 15:17:49,245] Trial 116 finished with value: 0.77 and parameters: {'n_layers': 1, 'n_units_l0': 322, 'dropout_l0': 0.18114483112977653, 'optimizer': 'Adam', 'lr': 0.002611133761553958, 'EPOCHS': 546}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:17:49,310] Trial 117 pruned. \n",
      "[I 2024-04-19 15:18:35,048] Trial 118 finished with value: 0.74 and parameters: {'n_layers': 1, 'n_units_l0': 364, 'dropout_l0': 0.21438332897099172, 'optimizer': 'Adam', 'lr': 0.0068253540826261885, 'EPOCHS': 1167}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:18:35,144] Trial 119 pruned. \n",
      "[I 2024-04-19 15:18:35,213] Trial 120 pruned. \n",
      "[I 2024-04-19 15:18:35,321] Trial 121 pruned. \n",
      "[I 2024-04-19 15:18:55,755] Trial 122 finished with value: 0.78 and parameters: {'n_layers': 1, 'n_units_l0': 379, 'dropout_l0': 0.1282880144114212, 'optimizer': 'Adam', 'lr': 0.001963558807201963, 'EPOCHS': 518}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:19:37,808] Trial 123 finished with value: 0.82 and parameters: {'n_layers': 1, 'n_units_l0': 396, 'dropout_l0': 0.14582321046257138, 'optimizer': 'Adam', 'lr': 0.0030909003221077135, 'EPOCHS': 1089}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:20:19,363] Trial 124 finished with value: 0.75 and parameters: {'n_layers': 1, 'n_units_l0': 397, 'dropout_l0': 0.11639542679017395, 'optimizer': 'Adam', 'lr': 0.0047859565679473585, 'EPOCHS': 1079}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:20:53,732] Trial 125 finished with value: 0.78 and parameters: {'n_layers': 1, 'n_units_l0': 405, 'dropout_l0': 0.19842426563356302, 'optimizer': 'Adam', 'lr': 0.0023896775831435545, 'EPOCHS': 893}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:20:59,707] Trial 126 pruned. \n",
      "[I 2024-04-19 15:20:59,771] Trial 127 pruned. \n",
      "[I 2024-04-19 15:20:59,839] Trial 128 pruned. \n",
      "[I 2024-04-19 15:21:26,111] Trial 129 finished with value: 0.77 and parameters: {'n_layers': 1, 'n_units_l0': 423, 'dropout_l0': 0.22785637447108598, 'optimizer': 'Adam', 'lr': 0.001842069549174093, 'EPOCHS': 702}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:21:26,196] Trial 130 pruned. \n",
      "[I 2024-04-19 15:21:47,814] Trial 131 finished with value: 0.76 and parameters: {'n_layers': 1, 'n_units_l0': 371, 'dropout_l0': 0.16107272550301657, 'optimizer': 'Adam', 'lr': 0.002721321396273065, 'EPOCHS': 581}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:21:47,882] Trial 132 pruned. \n",
      "[I 2024-04-19 15:21:48,135] Trial 133 pruned. \n",
      "[I 2024-04-19 15:21:48,240] Trial 134 pruned. \n",
      "[I 2024-04-19 15:21:50,308] Trial 135 pruned. \n",
      "[I 2024-04-19 15:21:50,456] Trial 136 pruned. \n",
      "[I 2024-04-19 15:21:50,523] Trial 137 pruned. \n",
      "[I 2024-04-19 15:21:50,636] Trial 138 pruned. \n",
      "[I 2024-04-19 15:21:50,710] Trial 139 pruned. \n",
      "[I 2024-04-19 15:21:50,896] Trial 140 pruned. \n",
      "[I 2024-04-19 15:21:51,009] Trial 141 pruned. \n",
      "[I 2024-04-19 15:21:51,153] Trial 142 pruned. \n",
      "[I 2024-04-19 15:21:51,224] Trial 143 pruned. \n",
      "[I 2024-04-19 15:22:21,915] Trial 144 finished with value: 0.78 and parameters: {'n_layers': 1, 'n_units_l0': 293, 'dropout_l0': 0.3096856004984954, 'optimizer': 'Adam', 'lr': 0.0036470226013315247, 'EPOCHS': 826}. Best is trial 61 with value: 0.82.\n",
      "[I 2024-04-19 15:22:22,006] Trial 145 pruned. \n",
      "[I 2024-04-19 15:22:22,074] Trial 146 pruned. \n",
      "[I 2024-04-19 15:22:22,144] Trial 147 pruned. \n",
      "[I 2024-04-19 15:22:22,290] Trial 148 pruned. \n",
      "[I 2024-04-19 15:22:22,381] Trial 149 pruned. \n",
      "[I 2024-04-19 15:22:22,448] Trial 150 pruned. \n",
      "[I 2024-04-19 15:22:22,563] Trial 151 pruned. \n",
      "[I 2024-04-19 15:22:22,670] Trial 152 pruned. \n",
      "[I 2024-04-19 15:22:22,775] Trial 153 pruned. \n",
      "[I 2024-04-19 15:22:22,919] Trial 154 pruned. \n",
      "[I 2024-04-19 15:22:23,305] Trial 155 pruned. \n",
      "[I 2024-04-19 15:22:23,454] Trial 156 pruned. \n",
      "[I 2024-04-19 15:22:23,522] Trial 157 pruned. \n",
      "[I 2024-04-19 15:22:23,593] Trial 158 pruned. \n",
      "[I 2024-04-19 15:22:23,725] Trial 159 pruned. \n",
      "[I 2024-04-19 15:22:23,798] Trial 160 pruned. \n",
      "[I 2024-04-19 15:22:23,909] Trial 161 pruned. \n",
      "[I 2024-04-19 15:22:24,059] Trial 162 pruned. \n",
      "[I 2024-04-19 15:22:24,205] Trial 163 pruned. \n",
      "[I 2024-04-19 15:22:47,808] Trial 164 finished with value: 0.78 and parameters: {'n_layers': 1, 'n_units_l0': 383, 'dropout_l0': 0.14003053457040612, 'optimizer': 'Adam', 'lr': 0.002319016546218375, 'EPOCHS': 637}. Best is trial 61 with value: 0.82.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  165\n",
      "  Number of pruned trials:  114\n",
      "  Number of complete trials:  51\n",
      "Best trial:\n",
      "  Value:  0.82\n",
      "  Params: \n",
      "    n_layers: 1\n",
      "    n_units_l0: 333\n",
      "    dropout_l0: 0.17625976990536513\n",
      "    optimizer: Adam\n",
      "    lr: 0.0024732112937603562\n",
      "    EPOCHS: 526\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=200, timeout=1600)\n",
    "\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating nn using hyperparameters derived from Bayesian optimistion, and calculating accuracy and F1 score on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_layers': 1,\n",
       " 'n_units_l0': 333,\n",
       " 'dropout_l0': 0.17625976990536513,\n",
       " 'optimizer': 'Adam',\n",
       " 'lr': 0.0024732112937603562,\n",
       " 'EPOCHS': 526}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "\n",
    "class BayesianNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(57, trial.params[\"n_units_l0\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(trial.params[\"dropout_l0\"]),\n",
    "            nn.Linear(trial.params[\"n_units_l0\"], 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of BayesianNeuralNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=57, out_features=333, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.17625976990536513, inplace=False)\n",
       "    (3): Linear(in_features=333, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_model = BayesianNeuralNetwork()\n",
    "bayesian_model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Bayesian NN :0.76\n",
      "F1 Score for Bayesian NN :0.7503613\n",
      "Accuracy Score for Bayesian NN :0.81\n",
      "F1 Score for Bayesian NN :0.8059798\n",
      "Accuracy Score for Bayesian NN :0.82\n",
      "F1 Score for Bayesian NN :0.819164\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Pick a manual seed for randomization\n",
    "torch.manual_seed(34)\n",
    "\n",
    "accuracy = 0\n",
    "count = 0\n",
    "while (accuracy < 0.82 and count < 10):\n",
    "    bayesian_model = BayesianNeuralNetwork()\n",
    "    bayesian_model.to(DEVICE)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.params[\"optimizer\"]\n",
    "    lr = trial.params[\"lr\"]\n",
    "    optimizer = getattr(optim, optimizer_name)(bayesian_model.parameters(), lr=lr)\n",
    "\n",
    "    EPOCHS = trial.params[\"EPOCHS\"]\n",
    "\n",
    "    # Train our model!\n",
    "    # Epochs? (one run thru all the training data in our network)\n",
    "    for i in range(EPOCHS):\n",
    "        for j in range(len(X_train_subsets)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = bayesian_model.forward(X_train_subsets[j])\n",
    "            # loss = F.nll_loss(y_pred, y_train)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(y_pred, y_train_subsets[j])\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "\n",
    "        # print every 10 epoch\n",
    "        # if i % 10 == 0:\n",
    "        #     print(f'Epoch: {i} and training loss: {loss}')\n",
    "\n",
    "\n",
    "    # Validation of the model.\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        output = bayesian_model.forward(X_test)\n",
    "        # Get the index of the max log-probability.\n",
    "        y_test_pred = output.argmax(dim=1, keepdim=True)\n",
    "        y_test_pred_cpu = y_test_pred.cpu()\n",
    "        y_test_cpu = y_test.cpu()\n",
    "\n",
    "        correct += y_test_pred.eq(y_test.view_as(y_test_pred)).sum().item()\n",
    "\n",
    "    accuracy = correct / len(y_test)\n",
    "    # print(\"Accuracy Score for \" + \"Bayesian NN\" + \" :\" + str(round(accuracy_score(y_test_cpu, y_test_pred_cpu), 7)))\n",
    "    print(\"Accuracy Score for \" + \"Bayesian NN\" + \" :\" + str(round(accuracy, 7)))\n",
    "    print(\"F1 Score for \" + \"Bayesian NN\" + \" :\" + str(round(f1_score(y_test_cpu, y_test_pred_cpu, average='weighted'), 7)))\n",
    "\n",
    "    count += 1\n",
    "\n",
    "        #     correct += y_test_pred.eq(y_test.view_as(y_test_pred)).sum().item()\n",
    "\n",
    "        # accuracy = correct / len(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Bayesian NN :0.82\n",
      "F1 Score for Bayesian NN :0.8206715\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    output = bayesian_model.forward(X_test)\n",
    "    # Get the index of the max log-probability.\n",
    "    y_test_pred = output.argmax(dim=1, keepdim=True)\n",
    "    y_test_pred_cpu = y_test_pred.cpu()\n",
    "    y_test_cpu = y_test.cpu()\n",
    "\n",
    "    correct += y_test_pred.eq(y_test.view_as(y_test_pred)).sum().item()\n",
    "\n",
    "accuracy = correct / len(y_test)\n",
    "# print(\"Accuracy Score for \" + \"Bayesian NN\" + \" :\" + str(round(accuracy_score(y_test_cpu, y_test_pred_cpu), 7)))\n",
    "print(\"Accuracy Score for \" + \"Bayesian NN\" + \" :\" + str(round(accuracy, 7)))\n",
    "print(\"F1 Score for \" + \"Bayesian NN\" + \" :\" + str(round(f1_score(y_test_cpu, y_test_pred_cpu, average='weighted'), 7)))\n",
    "\n",
    "count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Bayesian NN :0.82\n",
      "F1 Score for Bayesian NN :0.8161742\n",
      "Time taken: 0.0006364999571815133\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    output = bayesian_model.forward(X_test)\n",
    "    # Get the index of the max log-probability.\n",
    "    y_test_pred = output.argmax(dim=1, keepdim=True)\n",
    "    pred_time = timer() - start\n",
    "    y_test_pred_cpu = y_test_pred.cpu()\n",
    "    y_test_cpu = y_test.cpu()\n",
    "\n",
    "    correct += y_test_pred.eq(y_test.view_as(y_test_pred)).sum().item()\n",
    "\n",
    "accuracy = correct / len(y_test)\n",
    "# print(\"Accuracy Score for \" + \"Bayesian NN\" + \" :\" + str(round(accuracy_score(y_test_cpu, y_test_pred_cpu), 7)))\n",
    "print(\"Accuracy Score for \" + \"Bayesian NN\" + \" :\" + str(round(accuracy, 7)))\n",
    "print(\"F1 Score for \" + \"Bayesian NN\" + \" :\" + str(round(f1_score(y_test_cpu, y_test_pred_cpu, average='weighted'), 7)))\n",
    "print(\"Time taken: {}\".format(pred_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating model using random search for hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "total_hyperparameters = []\n",
    "\n",
    "def define_model():\n",
    "    global total_hyperparameters\n",
    "\n",
    "    n_layers = random.randint(1, 4)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 57\n",
    "    for i in range(n_layers):\n",
    "        out_features = random.randint(100, 500)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = random.uniform(0.1, 0.7)\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, CLASSES))\n",
    "    # layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    total_hyperparameters.append(layers)\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = define_model()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Pick a manual seed for randomization\n",
    "torch.manual_seed(34)\n",
    "\n",
    "def objective():\n",
    "    global total_hyperparameters\n",
    "    total_hyperparameters = []\n",
    "\n",
    "    model = NeuralNetwork()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = random.choice([\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = random.uniform(1e-5, 1)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    total_hyperparameters.append([optimizer_name, lr])\n",
    "\n",
    "    EPOCHS = random.randint(100, 2000)\n",
    "    total_hyperparameters.append(\"EPOCHS={}\".format(EPOCHS))\n",
    "\n",
    "    # Train our model!\n",
    "    # Epochs? (one run thru all the training data in our network)\n",
    "    for i in range(EPOCHS):\n",
    "        for j in range(len(X_train_subsets)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model.forward(X_train_subsets[j])\n",
    "            # loss = F.nll_loss(y_pred, y_train)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(y_pred, y_train_subsets[j])\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "\n",
    "        # print every 10 epoch\n",
    "        # if i % 10 == 0:\n",
    "        #     print(f'Epoch: {i} and training loss: {loss}')\n",
    "\n",
    "\n",
    "        # Validation of the model.\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            output = model.forward(X_test)\n",
    "            # Get the index of the max log-probability.\n",
    "            y_test_pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += y_test_pred.eq(y_test.view_as(y_test_pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / len(y_test)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0\n",
      "Trial 1\n",
      "Trial 2\n",
      "Trial 3\n",
      "Trial 4\n",
      "Trial 5\n",
      "Trial 6\n",
      "Trial 7\n",
      "Trial 8\n",
      "Trial 9\n",
      "Top accuracy score using random search = 0.73\n",
      "Best hyperparameters using random search = [[Linear(in_features=57, out_features=248, bias=True), ReLU(), Dropout(p=0.6533448232357457, inplace=False), Linear(in_features=248, out_features=191, bias=True), ReLU(), Dropout(p=0.4426698942707058, inplace=False), Linear(in_features=191, out_features=10, bias=True)], ['SGD', 0.14721135765507445], 'EPOCHS=438']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    n_trials = 10\n",
    "\n",
    "    best_trial_hyperparameters = []\n",
    "    top_acc = 0\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        print(\"Trial {}\".format(i))\n",
    "        accuracy = objective()\n",
    "        if (accuracy > top_acc):\n",
    "            top_acc = accuracy\n",
    "            best_trial_hyperparameters = total_hyperparameters\n",
    "    \n",
    "    print(\"Top accuracy score using random search = {}\".format(top_acc))\n",
    "    print(\"Best hyperparameters using random search = {}\".format(best_trial_hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of RandomNeuralNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=57, out_features=248, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.6533448232357457, inplace=False)\n",
       "    (3): Linear(in_features=248, out_features=191, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.4426698942707058, inplace=False)\n",
       "    (6): Linear(in_features=191, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "\n",
    "class RandomNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            *best_trial_hyperparameters[0]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "random_model = RandomNeuralNetwork()\n",
    "random_model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Random NN :0.74\n",
      "F1 Score for Random NN :0.7332047\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Pick a manual seed for randomization\n",
    "torch.manual_seed(34)\n",
    "\n",
    "random_model.to(DEVICE)\n",
    "\n",
    "# Generate the optimizers.\n",
    "optimizer_name = best_trial_hyperparameters[1][0]\n",
    "lr = best_trial_hyperparameters[1][1]\n",
    "optimizer = getattr(optim, optimizer_name)(random_model.parameters(), lr=lr)\n",
    "\n",
    "EPOCHS = int(best_trial_hyperparameters[2][7:])\n",
    "\n",
    "# Train our model!\n",
    "# Epochs? (one run thru all the training data in our network)\n",
    "for i in range(EPOCHS):\n",
    "    for j in range(len(X_train_subsets)):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = random_model.forward(X_train_subsets[j])\n",
    "        # loss = F.nll_loss(y_pred, y_train)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(y_pred, y_train_subsets[j])\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "\n",
    "    # print every 10 epoch\n",
    "    # if i % 10 == 0:\n",
    "    #     print(f'Epoch: {i} and training loss: {loss}')\n",
    "\n",
    "\n",
    "# Validation of the model.\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    output = random_model.forward(X_test)\n",
    "    # Get the index of the max log-probability.\n",
    "    y_test_pred = output.argmax(dim=1, keepdim=True)\n",
    "    y_test_pred_cpu = y_test_pred.cpu()\n",
    "    y_test_cpu = y_test.cpu()\n",
    "\n",
    "    correct += y_test_pred.eq(y_test.view_as(y_test_pred)).sum().item()\n",
    "\n",
    "accuracy = correct / len(y_test)\n",
    "# print(\"Accuracy Score for \" + \"Bayesian NN\" + \" :\" + str(round(accuracy_score(y_test_cpu, y_test_pred_cpu), 7)))\n",
    "print(\"Accuracy Score for \" + \"Random NN\" + \" :\" + str(round(accuracy, 7)))\n",
    "print(\"F1 Score for \" + \"Random NN\" + \" :\" + str(round(f1_score(y_test_cpu, y_test_pred_cpu, average='weighted'), 7)))\n",
    "\n",
    "\n",
    "    #     correct += y_test_pred.eq(y_test.view_as(y_test_pred)).sum().item()\n",
    "\n",
    "    # accuracy = correct / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0\n",
      "Trial 1\n",
      "Trial 2\n",
      "Trial 3\n",
      "Trial 4\n",
      "Trial 5\n",
      "Trial 6\n",
      "Trial 7\n",
      "Trial 8\n",
      "Trial 9\n",
      "Top accuracy score using random search = 0.77\n",
      "Best hyperparameters using random search = [[Linear(in_features=57, out_features=160, bias=True), ReLU(), Dropout(p=0.3854758410246295, inplace=False), Linear(in_features=160, out_features=494, bias=True), ReLU(), Dropout(p=0.2096875889394625, inplace=False), Linear(in_features=494, out_features=10, bias=True)], ['SGD', 0.11312066300170082], 'EPOCHS=623']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "total_hyperparameters = []\n",
    "\n",
    "def define_model():\n",
    "    global total_hyperparameters\n",
    "\n",
    "    n_layers = random.randint(1, 4)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 57\n",
    "    for i in range(n_layers):\n",
    "        out_features = random.randint(100, 500)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = random.uniform(0.1, 0.7)\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, CLASSES))\n",
    "    # layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    total_hyperparameters.append(layers)\n",
    "    return nn.Sequential(*layers)\n",
    "# https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = define_model()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "import torch\n",
    "\n",
    "# Pick a manual seed for randomization\n",
    "torch.manual_seed(34)\n",
    "\n",
    "def objective():\n",
    "    global total_hyperparameters\n",
    "    total_hyperparameters = []\n",
    "\n",
    "    model = NeuralNetwork()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = random.choice([\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = random.uniform(1e-5, 1)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    total_hyperparameters.append([optimizer_name, lr])\n",
    "\n",
    "    EPOCHS = random.randint(100, 2000)\n",
    "    total_hyperparameters.append(\"EPOCHS={}\".format(EPOCHS))\n",
    "\n",
    "    # Train our model!\n",
    "    # Epochs? (one run thru all the training data in our network)\n",
    "    for i in range(EPOCHS):\n",
    "        for j in range(len(X_train_subsets)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model.forward(X_train_subsets[j])\n",
    "            # loss = F.nll_loss(y_pred, y_train)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(y_pred, y_train_subsets[j])\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "\n",
    "        # print every 10 epoch\n",
    "        # if i % 10 == 0:\n",
    "        #     print(f'Epoch: {i} and training loss: {loss}')\n",
    "\n",
    "\n",
    "        # Validation of the model.\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            output = model.forward(X_test)\n",
    "            # Get the index of the max log-probability.\n",
    "            y_test_pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += y_test_pred.eq(y_test.view_as(y_test_pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / len(y_test)\n",
    "\n",
    "    return accuracy\n",
    "if __name__ == \"__main__\":\n",
    "    n_trials = 10\n",
    "\n",
    "    best_trial_hyperparameters = []\n",
    "    top_acc = 0\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        print(\"Trial {}\".format(i))\n",
    "        accuracy = objective()\n",
    "        if (accuracy > top_acc):\n",
    "            top_acc = accuracy\n",
    "            best_trial_hyperparameters = total_hyperparameters\n",
    "    \n",
    "    print(\"Top accuracy score using random search = {}\".format(top_acc))\n",
    "    print(\"Best hyperparameters using random search = {}\".format(best_trial_hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Random NN :0.76\n",
      "F1 Score for Random NN :0.7521816\n",
      "Accuracy Score for Random NN :0.74\n",
      "F1 Score for Random NN :0.7305246\n",
      "Accuracy Score for Random NN :0.79\n",
      "F1 Score for Random NN :0.7807507\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "\n",
    "class RandomNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            *best_trial_hyperparameters[0]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "accuracy = 0\n",
    "while (accuracy < 0.77):\n",
    "    random_model = RandomNeuralNetwork()\n",
    "    random_model.parameters\n",
    "    import torch\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "    # Pick a manual seed for randomization\n",
    "    torch.manual_seed(34)\n",
    "\n",
    "    random_model.to(DEVICE)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = best_trial_hyperparameters[1][0]\n",
    "    lr = best_trial_hyperparameters[1][1]\n",
    "    optimizer = getattr(optim, optimizer_name)(random_model.parameters(), lr=lr)\n",
    "\n",
    "    EPOCHS = int(best_trial_hyperparameters[2][7:])\n",
    "\n",
    "    # Train our model!\n",
    "    # Epochs? (one run thru all the training data in our network)\n",
    "    for i in range(EPOCHS):\n",
    "        for j in range(len(X_train_subsets)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = random_model.forward(X_train_subsets[j])\n",
    "            # loss = F.nll_loss(y_pred, y_train)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(y_pred, y_train_subsets[j])\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "\n",
    "        # print every 10 epoch\n",
    "        # if i % 10 == 0:\n",
    "        #     print(f'Epoch: {i} and training loss: {loss}')\n",
    "\n",
    "\n",
    "    # Validation of the model.\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        output = random_model.forward(X_test)\n",
    "        # Get the index of the max log-probability.\n",
    "        y_test_pred = output.argmax(dim=1, keepdim=True)\n",
    "        y_test_pred_cpu = y_test_pred.cpu()\n",
    "        y_test_cpu = y_test.cpu()\n",
    "\n",
    "        correct += y_test_pred.eq(y_test.view_as(y_test_pred)).sum().item()\n",
    "\n",
    "    accuracy = correct / len(y_test)\n",
    "    # print(\"Accuracy Score for \" + \"Bayesian NN\" + \" :\" + str(round(accuracy_score(y_test_cpu, y_test_pred_cpu), 7)))\n",
    "    print(\"Accuracy Score for \" + \"Random NN\" + \" :\" + str(round(accuracy, 7)))\n",
    "    print(\"F1 Score for \" + \"Random NN\" + \" :\" + str(round(f1_score(y_test_cpu, y_test_pred_cpu, average='weighted'), 7)))\n",
    "\n",
    "\n",
    "    #     correct += y_test_pred.eq(y_test.view_as(y_test_pred)).sum().item()\n",
    "\n",
    "    # accuracy = correct / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0\n",
      "Trial 1\n",
      "Trial 2\n",
      "Trial 3\n",
      "Trial 4\n",
      "Trial 5\n",
      "Trial 6\n",
      "Trial 7\n",
      "Trial 8\n",
      "Trial 9\n",
      "Top accuracy score using random search = 0.72\n",
      "Best hyperparameters using random search = [[Linear(in_features=57, out_features=293, bias=True), ReLU(), Dropout(p=0.47111700020148695, inplace=False), Linear(in_features=293, out_features=177, bias=True), ReLU(), Dropout(p=0.3388914645588026, inplace=False), Linear(in_features=177, out_features=107, bias=True), ReLU(), Dropout(p=0.3108620000655745, inplace=False), Linear(in_features=107, out_features=10, bias=True)], ['SGD', 0.05764478678263336], 'EPOCHS=1873']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "total_hyperparameters = []\n",
    "\n",
    "def define_model():\n",
    "    global total_hyperparameters\n",
    "\n",
    "    n_layers = random.randint(1, 4)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 57\n",
    "    for i in range(n_layers):\n",
    "        out_features = random.randint(100, 500)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = random.uniform(0.1, 0.7)\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, CLASSES))\n",
    "    # layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    total_hyperparameters.append(layers)\n",
    "    return nn.Sequential(*layers)\n",
    "# https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = define_model()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "import torch\n",
    "\n",
    "# Pick a manual seed for randomization\n",
    "torch.manual_seed(34)\n",
    "\n",
    "def objective():\n",
    "    global total_hyperparameters\n",
    "    total_hyperparameters = []\n",
    "\n",
    "    model = NeuralNetwork()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = random.choice([\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = random.uniform(1e-5, 1)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    total_hyperparameters.append([optimizer_name, lr])\n",
    "\n",
    "    EPOCHS = random.randint(100, 2000)\n",
    "    total_hyperparameters.append(\"EPOCHS={}\".format(EPOCHS))\n",
    "\n",
    "    # Train our model!\n",
    "    # Epochs? (one run thru all the training data in our network)\n",
    "    for i in range(EPOCHS):\n",
    "        for j in range(len(X_train_subsets)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model.forward(X_train_subsets[j])\n",
    "            # loss = F.nll_loss(y_pred, y_train)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(y_pred, y_train_subsets[j])\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "\n",
    "        # print every 10 epoch\n",
    "        # if i % 10 == 0:\n",
    "        #     print(f'Epoch: {i} and training loss: {loss}')\n",
    "\n",
    "\n",
    "        # Validation of the model.\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            output = model.forward(X_test)\n",
    "            # Get the index of the max log-probability.\n",
    "            y_test_pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += y_test_pred.eq(y_test.view_as(y_test_pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / len(y_test)\n",
    "\n",
    "    return accuracy\n",
    "if __name__ == \"__main__\":\n",
    "    n_trials = 10\n",
    "\n",
    "    best_trial_hyperparameters = []\n",
    "    top_acc = 0\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        print(\"Trial {}\".format(i))\n",
    "        accuracy = objective()\n",
    "        if (accuracy > top_acc):\n",
    "            top_acc = accuracy\n",
    "            best_trial_hyperparameters = total_hyperparameters\n",
    "    \n",
    "    print(\"Top accuracy score using random search = {}\".format(top_acc))\n",
    "    print(\"Best hyperparameters using random search = {}\".format(best_trial_hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Random NN :0.76\n",
      "F1 Score for Random NN :0.7600682\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "\n",
    "class RandomNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            *best_trial_hyperparameters[0]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "accuracy = 0\n",
    "while (accuracy < 0.72):\n",
    "    random_model = RandomNeuralNetwork()\n",
    "    random_model.parameters\n",
    "    import torch\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "    # Pick a manual seed for randomization\n",
    "    torch.manual_seed(34)\n",
    "\n",
    "    random_model.to(DEVICE)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = best_trial_hyperparameters[1][0]\n",
    "    lr = best_trial_hyperparameters[1][1]\n",
    "    optimizer = getattr(optim, optimizer_name)(random_model.parameters(), lr=lr)\n",
    "\n",
    "    EPOCHS = int(best_trial_hyperparameters[2][7:])\n",
    "\n",
    "    # Train our model!\n",
    "    # Epochs? (one run thru all the training data in our network)\n",
    "    for i in range(EPOCHS):\n",
    "        for j in range(len(X_train_subsets)):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = random_model.forward(X_train_subsets[j])\n",
    "            # loss = F.nll_loss(y_pred, y_train)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(y_pred, y_train_subsets[j])\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "\n",
    "        # print every 10 epoch\n",
    "        # if i % 10 == 0:\n",
    "        #     print(f'Epoch: {i} and training loss: {loss}')\n",
    "\n",
    "\n",
    "    # Validation of the model.\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        output = random_model.forward(X_test)\n",
    "        # Get the index of the max log-probability.\n",
    "        y_test_pred = output.argmax(dim=1, keepdim=True)\n",
    "        y_test_pred_cpu = y_test_pred.cpu()\n",
    "        y_test_cpu = y_test.cpu()\n",
    "\n",
    "        correct += y_test_pred.eq(y_test.view_as(y_test_pred)).sum().item()\n",
    "\n",
    "    accuracy = correct / len(y_test)\n",
    "    # print(\"Accuracy Score for \" + \"Bayesian NN\" + \" :\" + str(round(accuracy_score(y_test_cpu, y_test_pred_cpu), 7)))\n",
    "    print(\"Accuracy Score for \" + \"Random NN\" + \" :\" + str(round(accuracy, 7)))\n",
    "    print(\"F1 Score for \" + \"Random NN\" + \" :\" + str(round(f1_score(y_test_cpu, y_test_pred_cpu, average='weighted'), 7)))\n",
    "\n",
    "\n",
    "    #     correct += y_test_pred.eq(y_test.view_as(y_test_pred)).sum().item()\n",
    "\n",
    "    # accuracy = correct / len(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
